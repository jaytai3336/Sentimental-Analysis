{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sentimental Analysis\n",
    "\n",
    "Goal: Scrape data from a website like bloomberg (all the bolded words), separate the news into categories, then assign a sentimental value\n",
    "\n",
    "spaCY was used to do preprocessing\n",
    "\n",
    "The following library are to be explored:\n",
    "1. VADER\n",
    "2. TextBlob\n",
    "3. Flair\n",
    "4. Models - RoBERTA (HuggingFace), DistilliBERT (HuggingFace)\n",
    "5. LLM\n",
    "6. Self Built (self-sourced Dataset)\n",
    "\n",
    "Plan:\n",
    "1. Compare between models 1-5\n",
    "2. Using the best model available to assign values to the data\n",
    "3. train own model using dataset\n",
    "4. Compare and conclude"
   ],
   "id": "2193c22260401624"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exploring the Py libraries",
   "id": "f0bd2e9beb7fc1ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "49b31de351000735",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# setup\n",
    "\n",
    "sentence = \"Trump to Leave G-7 Tonight Due to Middle East Crisis\"\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str) or text is None:\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    test_sentence = \" \".join(cleaned_tokens)\n",
    "    return test_sentence\n",
    "\n",
    "test_sentence = preprocess(sentence)"
   ],
   "id": "9fac6435f5ff0170",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. VADER",
   "id": "7a7d37fc0c0f14fc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prebuilt Vader sentiment package\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(test_sentence)\n",
    "print(scores)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. TextBlob",
   "id": "119bdd34da09b06a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prebuilt Textblob sentiment package\n",
    "\n",
    "from textblob import TextBlob\n",
    "text = TextBlob(test_sentence)\n",
    "score = text.sentiment\n",
    "print(score)"
   ],
   "id": "91d3e6559d45db91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Flair\n",
    "\n",
    "Is optimized for sequence labeling but also has prebuild sentiment classification"
   ],
   "id": "5c513bc60f546da9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prebuilt Flair sentiment package/Model\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "\n",
    "sentence = Sentence(test_sentence)\n",
    "tagger = Classifier.load('sentiment')\n",
    "tagger.predict(sentence)\n",
    "print(sentence)"
   ],
   "id": "3a17b17ba7907748",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. HuggingFace Transformers",
   "id": "17218a63e9f9044c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T14:41:17.988132Z",
     "start_time": "2025-06-17T14:41:17.931730Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import pipeline",
   "id": "87c16e128c2783db",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### - RoBERTa",
   "id": "cb40705550143cc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "classifier = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment-latest')",
   "id": "abcf26e6644caaef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for sentence in processed_headine:\n",
    "    score = 0\n",
    "    temp = classifier(sentence)\n",
    "    if temp['label'] == 'positive': score = temp[0]['score']\n",
    "    elif temp['label'] == 'negative': score = -temp[0]['score']\n",
    "    result_roberta.append(score)\n",
    "    \n",
    "result = classifier(\"hi hi no\")\n",
    "print(result)"
   ],
   "id": "5858e555d5fcb62b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### - DistilBERT",
   "id": "4acdd1f0a2de7930"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T14:45:01.718528Z",
     "start_time": "2025-06-17T14:44:17.308540Z"
    }
   },
   "cell_type": "code",
   "source": "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')",
   "id": "68f02a1362c85665",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b22fd79be58341649fb942efd9ce262d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc284e4ca3764a179d591c3ec659d460"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50932244f9c041568760afb63fbc9d70"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "716c83c6ec3941ce9430489904b88be4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T14:47:46.192265Z",
     "start_time": "2025-06-17T14:47:40.260173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = classifier(\"no no no never\")\n",
    "print(result)"
   ],
   "id": "5d14df24a81bf60b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9756912589073181}]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### - Google Flan t5 base LLM model (Open source via HuggingFace)",
   "id": "b1be917b6a3da9b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-17T14:51:53.063695Z"
    }
   },
   "cell_type": "code",
   "source": "classifier = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")",
   "id": "d8fe64f25bbd57c8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87636419b7c14f22a08e1011c4c1a1e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa049778078640e1a09c1ef4c72444c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = f\"Classify the sentiment of '{\"Hi Hi\"}' as positive, negative, or neutral, and give a sentimental score of -1 to 1.\"\n",
    "result = classifier(prompt)\n",
    "print(result)"
   ],
   "id": "59233ff9167ec835"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. OpenRouter to send api request to LLM",
   "id": "ad5d025246605ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LLM Qwen\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=\"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\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  extra_body={},\n",
    "  model=\"deepseek/deepseek-r1-0528-qwen3-8b:free\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": f\"Conduct Sentimental Analysis on the following statement(s) and give me a polarity and score. {test_sentence}\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "id": "c7a8d51013f358ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Building own model from data taken from Kraggle\n",
    "\n",
    "cnbc: (3080, 3) -- reserved for testing and backtesting\n",
    "\n",
    "guardian: (17800, 2)\n",
    "+\n",
    "retuers: (32770, 3)\n",
    "\n",
    "I pass the data into an ai to generate a list of sentimental values to be assigned (lazy to self assign)\n",
    "\n",
    "train-split 80-20 and trained"
   ],
   "id": "ded858992243b6d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "cnbc = pd.read_csv(\"data/cnbc_headlines.csv\") # used as a testing data across the board\n",
    "guardian = pd.read_csv(\"data/guardian_headlines.csv\")\n",
    "reuters = pd.read_csv(\"data/reuters_headlines.csv\")"
   ],
   "id": "1bf7f406b267cde8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# preparing training and testing data\n",
    "\n",
    "import random\n",
    "random.seed(999)\n",
    "data = list(map(preprocess, (list(guardian['Headlines']) + list(reuters['Headlines']) + list(cnbc['Headlines']))))\n",
    "random.shuffle(data)\n",
    "t = int(len(data)*.8)\n",
    "train_set = data[:t]\n",
    "test_set = data[t:]"
   ],
   "id": "ec2b33bf8fc924eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(train_set), len(test_set)",
   "id": "dcc4ad0fc36126b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We create a preliminary model using NLTK library trained on our dataset\n",
    "\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "a = SentimentAnalyzer()\n"
   ],
   "id": "c6e9c4a3ea2e0712",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eb0f4f96ace7ff39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ec96c29ad9634bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "94cb891652d452bf",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
