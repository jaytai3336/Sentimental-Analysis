{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c44a31e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/News Articles/raw/trump_social_results2.csv')\n",
    "tweets = df['text'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "958003e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jay\n",
      "[nltk_data]     Tai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Jay Tai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to C:\\Users\\Jay\n",
      "[nltk_data]     Tai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Jay\n",
      "[nltk_data]     Tai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Jay\n",
      "[nltk_data]     Tai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jay\n",
      "[nltk_data]     Tai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0cc721f5-a5d0-4c00-b26e-f9532f49f201",
       "rows": [
        [
         "0",
         "I am pleased to announce that I am directing our GREAT Secretary of Transportation, Sean Duffy, to be Interim Administrator of NASA. Sean is doing a TREMENDOUS job in handling our Country’s Transportation Affairs, including creating a state-of-the-art Air Traffic Control systems, while at the same time rebuilding our roads and bridges, making them efficient, and beautiful, again. He will be a fantastic leader of the ever more important Space Agency, even if only for a short period of time. Congratulations, and thank you, Sean!"
        ],
        [
         "1",
         "I am announcing a 50% TARIFF on Copper, effective August 1, 2025, after receiving a robust NATIONAL SECURITY ASSESSMENT. Copper is necessary for Semiconductors, Aircraft, Ships, Ammunition, Data Centers, Lithium-ion Batteries, Radar Systems, Missile Defense Systems, and even, Hypersonic Weapons, of which we are building many. Copper is the second most used material by the Department of Defense! Why did our foolish (and SLEEPY!) “Leaders” decimate this important Industry? This 50% TARIFF will reverse the Biden Administration’s thoughtless behavior, and stupidity. America will, once again, build a DOMINANT Copper Industry. THIS IS, AFTER ALL, OUR GOLDEN AGE!"
        ],
        [
         "2",
         "HAPPY BIRTHDAY TO SENATOR LINDSEY GRAHAM! He is always there when I need him, and I hope everyone in the Great State of South Carolina will help Lindsey have a BIG WIN in his Re-Election bid next year. MAKE AMERICA GREAT AGAIN!"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3
       }
      },
      "text/plain": [
       "0    I am pleased to announce that I am directing o...\n",
       "1    I am announcing a 50% TARIFF on Copper, effect...\n",
       "2    HAPPY BIRTHDAY TO SENATOR LINDSEY GRAHAM! He i...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"maxent_ne_chunker_tab\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words('english')).union({\n",
    "    \"said\", \"mr\", \"u\", \"s\", \"today\", \"report\", \"according\"\n",
    "})\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "financial_phrases = [\n",
    "    \"beats expectations\", \"misses expectations\", \"strong guidance\",\n",
    "    \"weak guidance\", \"downgraded rating\", \"upgraded rating\",\n",
    "    \"raises outlook\", \"cuts outlook\", \"missed earnings\", \n",
    "    \"beat earnings\", \"profit warning\", \"record profits\",\n",
    "    \"trading halt\", \"surprise loss\", \"share buyback\", \"stock split\",\n",
    "    \"positive forecast\", \"negative forecast\", \"unexpected loss\"\n",
    "]\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    named_entities = []\n",
    "    for subtree in chunked:\n",
    "        if isinstance(subtree, Tree):\n",
    "            entity = \" \".join(token for token, pos in subtree.leaves())\n",
    "            named_entities.append(entity.lower())\n",
    "    return named_entities\n",
    "\n",
    "def extract_financial_phrases(text):\n",
    "    phrases_found = []\n",
    "    for phrase in financial_phrases:\n",
    "        # Allow variable whitespace, case-insensitive\n",
    "        pattern = r\"\\b\" + r\"\\s+\".join(re.escape(word) for word in phrase.split()) + r\"\\b\"\n",
    "        match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            phrases_found.append(match.group(0).lower())\n",
    "    return phrases_found\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    text = text.strip()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    named_entities = extract_named_entities(text)\n",
    "    fin_phrases = extract_financial_phrases(text)\n",
    "\n",
    "    all_preserved = set(named_entities + fin_phrases)\n",
    "\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    other_tokens = [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token.isalpha() and token not in stop_words and token not in \" \".join(all_preserved)\n",
    "    ]\n",
    "\n",
    "    return \" \".join(fin_phrases + named_entities + other_tokens)\n",
    "\n",
    "\n",
    "processed_tweets = list(map(preprocess, tweets))\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c607e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUPS = {\n",
    "    \"Financial Markets\": [\n",
    "        \"stocks\", \"equities\", \"shares\", \"indices\", \"dow\", \"nasdaq\", \"s&p\",\n",
    "        \"bonds\", \"treasuries\", \"yield\", \"spread\", \"options\", \"futures\",\n",
    "        \"margin\", \"leverage\", \"short\", \"dividends\", \"etf\", \"volatility\", \n",
    "        \"vix\", \"hedge\", \"trading\", \"liquidity\", \"volume\", \"order book\", \n",
    "        \"forecast\", \"price action\", \"open interest\", \"technicals\"\n",
    "    ],\n",
    "    \"Corporate Sector\": [\n",
    "        \"earnings\", \"revenue\", \"guidance\", \"layoffs\", \"acquisition\", \"merger\", \n",
    "        \"ipo\", \"startup\", \"valuation\", \"unicorn\", \"restructuring\", \"subsidiary\",\n",
    "        \"brand\", \"expansion\", \"subsidiary\", \"joint venture\", \"conglomerate\",\n",
    "        \"company\", \"profits\", \"CEO\", \"management\", \"shareholder\", \"stake\"\n",
    "    ],\n",
    "    \"Macro Finance\": [\n",
    "        \"inflation\", \"interest\", \"rate hike\", \"fed\", \"ecb\", \"central bank\", \n",
    "        \"monetary\", \"liquidity\", \"policy\", \"credit\", \"debt\", \"balance sheet\", \n",
    "        \"yields\", \"bond buying\", \"qe\", \"qt\", \"macro\", \"gdp\", \"growth\", \"cpi\", \n",
    "        \"ppi\", \"unemployment\", \"deficit\", \"surplus\", \"sovereign\", \"treasury\"\n",
    "    ],\n",
    "    \"Energy & Commodities\": [\n",
    "        \"oil\", \"brent\", \"wti\", \"crude\", \"natural gas\", \"coal\", \"uranium\", \n",
    "        \"gold\", \"silver\", \"copper\", \"commodity\", \"minerals\", \"barrel\", \"supply\",\n",
    "        \"demand\", \"refinery\", \"pipeline\", \"opec\", \"inventory\", \"mining\", \n",
    "        \"extraction\", \"energy\", \"power\", \"grid\", \"electricity\"\n",
    "    ],\n",
    "    \"Geopolitics & Policy\": [\n",
    "        \"war\", \"conflict\", \"military\", \"sanctions\", \"elections\", \"diplomacy\", \n",
    "        \"tariffs\", \"regime\", \"treaty\", \"un\", \"nato\", \"geopolitics\", \"alliance\", \n",
    "        \"border\", \"summit\", \"foreign policy\", \"cyberwar\", \"espionage\", \"blockade\"\n",
    "    ],\n",
    "    \"Tech & Innovation\": [\n",
    "        \"ai\", \"machine learning\", \"deep learning\", \"chatbot\", \"quantum\", \n",
    "        \"robotics\", \"semiconductors\", \"chips\", \"hardware\", \"software\", \"cloud\", \n",
    "        \"infrastructure\", \"cybersecurity\", \"5g\", \"6g\", \"platform\", \"saas\", \n",
    "        \"startup\", \"innovation\", \"automation\", \"big data\", \"iot\", \"virtual reality\"\n",
    "    ],\n",
    "    \"Crypto & Digital Assets\": [\n",
    "        \"crypto\", \"bitcoin\", \"ethereum\", \"nft\", \"token\", \"defi\", \"stablecoin\", \n",
    "        \"blockchain\", \"mining\", \"wallet\", \"exchange\", \"smart contract\", \n",
    "        \"airdrops\", \"gas fees\", \"halving\", \"web3\", \"metaverse\", \"yield farming\"\n",
    "    ],\n",
    "    \"Climate & Environment\": [\n",
    "        \"climate\", \"global warming\", \"carbon\", \"emissions\", \"green\", \n",
    "        \"net zero\", \"biodiversity\", \"deforestation\", \"renewables\", \"solar\", \n",
    "        \"wind\", \"sustainability\", \"eco\", \"climate risk\", \"co2\", \"enviro\", \n",
    "        \"recycling\", \"pollution\", \"weather\", \"drought\", \"wildfire\", \"hurricane\"\n",
    "    ],\n",
    "    \"Health & Biotech\": [\n",
    "        \"healthcare\", \"pharma\", \"vaccine\", \"covid\", \"pandemic\", \"epidemic\", \n",
    "        \"hospital\", \"insurance\", \"fda\", \"biotech\", \"drug\", \"therapy\", \n",
    "        \"clinical trials\", \"approval\", \"genomics\", \"mrna\", \"public health\"\n",
    "    ],\n",
    "    \"Consumer & Retail\": [\n",
    "        \"retail\", \"e-commerce\", \"spending\", \"shopping\", \"consumer\", \"foot traffic\", \n",
    "        \"brand\", \"loyalty\", \"promotion\", \"fashion\", \"luxury\", \"discount\", \n",
    "        \"travel\", \"leisure\", \"supermarket\", \"apparel\", \"inventory\", \"mall\", \n",
    "        \"tourism\", \"holiday sales\", \"storefront\", \"lifestyle\"\n",
    "    ],\n",
    "    \"Society & Labor\": [\n",
    "        \"labor\", \"employment\", \"wages\", \"strike\", \"union\", \"pension\", \n",
    "        \"migration\", \"education\", \"healthcare\", \"inequality\", \"crime\", \n",
    "        \"working class\", \"jobless\", \"benefits\", \"minimum wage\", \"social unrest\"\n",
    "    ],\n",
    "    \"Legal & Regulation\": [\n",
    "        \"regulation\", \"compliance\", \"litigation\", \"lawsuit\", \"ban\", \n",
    "        \"fine\", \"sec\", \"doj\", \"fca\", \"privacy\", \"antitrust\", \"audit\", \n",
    "        \"whistleblower\", \"ethics\", \"governance\", \"oversight\"\n",
    "    ],\n",
    "    \"Real Estate & Housing\": [\n",
    "        \"housing\", \"mortgage\", \"real estate\", \"rent\", \"home sales\", \n",
    "        \"construction\", \"property\", \"housing market\", \"zoning\", \"commercial real estate\", \n",
    "        \"land\", \"housing prices\", \"tenant\", \"eviction\", \"housing bubble\"\n",
    "    ],\n",
    "    \"Transport & Logistics\": [\n",
    "        \"shipping\", \"freight\", \"logistics\", \"supply chain\", \"port\", \n",
    "        \"airline\", \"aviation\", \"rail\", \"infrastructure\", \"trucking\", \n",
    "        \"transportation\", \"container\", \"cargo\", \"disruption\", \"delivery\"\n",
    "    ],\n",
    "    \"Education & Knowledge\": [\n",
    "        \"school\", \"university\", \"student\", \"curriculum\", \"exam\", \"scholarship\", \n",
    "        \"tuition\", \"degree\", \"research\", \"academic\", \"professor\", \n",
    "        \"online learning\", \"edtech\", \"education policy\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af48620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Analyzing tweets:   0%|          | 0/3 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Analyzing tweets:  67%|██████▋   | 2/3 [01:12<00:36, 36.34s/it]"
     ]
    }
   ],
   "source": [
    "# Extended Tweet Classification Pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, set_seed, AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.cluster import KMeans\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Load NLP models\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/deberta-v3-base-zeroshot-v1.1-all-33\", device=0 if torch.cuda.is_available() else -1)\n",
    "sentiment = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=0 if torch.cuda.is_available() else -1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embed_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Helper to extract contextual embeddings\n",
    "def get_embedding(text):\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(embed_model.device)\n",
    "    with torch.no_grad():\n",
    "        model_output = embed_model(**encoded_input)\n",
    "    return model_output.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# Multi-label classifier setup\n",
    "def multi_label_classification(text, candidate_labels, threshold=0.5):\n",
    "    result = classifier(text, candidate_labels=candidate_labels, multi_label=True)\n",
    "    labels = [label for label, score in zip(result['labels'], result['scores']) if score >= threshold]\n",
    "    return labels, result\n",
    "\n",
    "# Data processing\n",
    "results = []\n",
    "embeddings = []\n",
    "\n",
    "for sentence in tqdm(processed_tweets, desc=\"Analyzing tweets\"):\n",
    "    try:\n",
    "        # Topic classification\n",
    "        topics, full_result = multi_label_classification(sentence, list(GROUPS.keys()), threshold=0.5)\n",
    "\n",
    "        # Named entity recognition\n",
    "        doc = nlp(sentence)\n",
    "        named_entities = list(set(ent.text for ent in doc.ents))\n",
    "\n",
    "        # Sentiment\n",
    "        sent_result = sentiment(sentence)[0]\n",
    "\n",
    "        # Keyword matching\n",
    "        matched_keywords = []\n",
    "        for label in topics:\n",
    "            matched_keywords.extend([kw for kw in GROUPS[label] if re.search(rf\"\\\\b{re.escape(kw)}\\\\b\", sentence.lower())])\n",
    "\n",
    "        # Embedding\n",
    "        emb = get_embedding(sentence)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "        results.append({\n",
    "            \"text\": sentence,\n",
    "            \"topics\": topics if topics else [\"Uncertain\"],\n",
    "            \"top_3_topics\": \", \".join(full_result['labels'][:3]),\n",
    "            \"topic_confidences\": [round(score, 3) for score in full_result['scores'][:3]],\n",
    "            \"matched_keywords\": \", \".join(matched_keywords[:3]) if matched_keywords else \"None\",\n",
    "            \"sentiment\": sent_result['label'].upper(),\n",
    "            \"sentiment_score\": round(sent_result['score'], 4),\n",
    "            \"named_entities\": named_entities\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {sentence[:50]}... -> {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Embedding clustering\n",
    "X = np.stack(embeddings)\n",
    "kmeans = KMeans(n_clusters=6, random_state=42).fit(X)\n",
    "results_df['cluster'] = kmeans.labels_\n",
    "\n",
    "# Binarize multi-label topics\n",
    "mlb = MultiLabelBinarizer()\n",
    "topic_binarized = pd.DataFrame(mlb.fit_transform(results_df['topics']), columns=mlb.classes_)\n",
    "results_df = pd.concat([results_df, topic_binarized], axis=1)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(\"../data/News Articles/processed/tweet_analysis_extended.csv\", index=False)\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(data=results_df, x='cluster', hue='sentiment')\n",
    "plt.title(\"Sentiment Distribution by Cluster\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
