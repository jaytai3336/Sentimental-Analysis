{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf03c66dcfb800b",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "trained using the same 2000 entries. train-test split 8-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T06:38:03.251003Z",
     "start_time": "2025-06-30T06:38:03.221848Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Index                                           Headline Sentiment_label\n",
      "0      0   Johnson is asking Santa for a Christmas recovery             POS\n",
      "1      1  ‘I now fear the worst’: four grim tales of wor...             NEG\n",
      "2      2  Five key areas Sunak must tackle to serve up e...             NEU\n",
      "3      3  Covid-19 leaves firms ‘fatally ill-prepared’ f...             NEG\n",
      "4      4  The Week in Patriarchy Bacardi's 'lady vodka':...             NEG\n",
      "             Index\n",
      "count  1984.000000\n",
      "mean    993.127520\n",
      "std     575.397447\n",
      "min       0.000000\n",
      "25%     495.750000\n",
      "50%     991.500000\n",
      "75%    1487.250000\n",
      "max    1999.000000\n"
     ]
    }
   ],
   "source": [
    "# labelling all the data using the best model: distilbert\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/News Articles/Grok/headlines_with_sentiment.csv\")\n",
    "print(df.head())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1698d4c183718dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T06:38:06.322921Z",
     "start_time": "2025-06-30T06:38:04.083673Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Jay\n",
      "[nltk_data]     Tai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jay\n",
      "[nltk_data]     Tai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Jay\n",
      "[nltk_data]     Tai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Headline",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Sentiment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7aab0dac-e8c7-41e4-b1ab-5ec5020df102",
       "rows": [
        [
         "0",
         "johnson asking santa christmas recovery",
         "POS"
        ],
        [
         "1",
         "fear worst four grim tale working life upended",
         "NEG"
        ],
        [
         "2",
         "five key area sunak must tackle serve economic recovery",
         "NEU"
        ],
        [
         "3",
         "leaf firm fatally brexit",
         "NEG"
        ],
        [
         "4",
         "week patriarchy bacardi vodka latest long line depressing gendered product",
         "NEG"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>johnson asking santa christmas recovery</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fear worst four grim tale working life upended</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>five key area sunak must tackle serve economic...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leaf firm fatally brexit</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>week patriarchy bacardi vodka latest long line...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline Sentiment\n",
       "0            johnson asking santa christmas recovery       POS\n",
       "1     fear worst four grim tale working life upended       NEG\n",
       "2  five key area sunak must tackle serve economic...       NEU\n",
       "3                           leaf firm fatally brexit       NEG\n",
       "4  week patriarchy bacardi vodka latest long line...       NEG"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str) or text is None:\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    test_sentence = \" \".join(cleaned_tokens)\n",
    "    return test_sentence\n",
    "\n",
    "headline = list(df['Headline'])\n",
    "processed_headline = list(map(preprocess, headline))\n",
    "\n",
    "data = pd.DataFrame({'Headline': processed_headline, 'Sentiment': df['Sentiment_label'] })\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc313f4641288f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T06:38:08.407110Z",
     "start_time": "2025-06-30T06:38:08.399113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1587,)\n",
      "X_test shape: (397,)\n",
      "y_train shape: (1587,)\n",
      "y_test shape: (397,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset  into test and train\n",
    "# 90% train , 10% test and random state 999\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.random.seed(999)\n",
    "\n",
    "# Define the features and target variable\n",
    "X = data['Headline']  # Features\n",
    "y = data['Sentiment']  # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=999)\n",
    "\n",
    "# Print the shape of the training and testing sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d17846b2998f40c",
   "metadata": {},
   "source": [
    "## Testing using different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "685a6f12dea3b7b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T06:38:11.771553Z",
     "start_time": "2025-06-30T06:38:11.713457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'POS' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG'\n",
      " 'POS' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEU' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEU'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEU' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEU' 'NEU' 'NEG' 'NEG' 'NEU' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEU' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEU' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEU' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEU' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'POS' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG']\n",
      "LinearSVC\n",
      "accuracy score: 89.92%\n",
      "Confusion Matrix:\n",
      " [[338   3   5]\n",
      " [  9   5   1]\n",
      " [ 20   2  14]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.92      0.98      0.95       346\n",
      "         NEU       0.50      0.33      0.40        15\n",
      "         POS       0.70      0.39      0.50        36\n",
      "\n",
      "    accuracy                           0.90       397\n",
      "   macro avg       0.71      0.57      0.62       397\n",
      "weighted avg       0.89      0.90      0.89       397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('model', LinearSVC())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"LinearSVC\")\n",
    "\n",
    "print(\"accuracy score: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe83609c0247d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T06:37:36.875852600Z",
     "start_time": "2025-06-19T07:18:39.722775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "accuracy score: 86.93%\n",
      "Confusion Matrix:\n",
      " [[173   0   0]\n",
      " [  9   0   0]\n",
      " [ 17   0   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.87      1.00      0.93       173\n",
      "         NEU       0.00      0.00      0.00         9\n",
      "         POS       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.87       199\n",
      "   macro avg       0.29      0.33      0.31       199\n",
      "weighted avg       0.76      0.87      0.81       199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay Tai\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Jay Tai\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Jay Tai\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# LR\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Create a pipeline with CountVectorizer, TfidfTransformer, and Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test dataset\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"accuracy score: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "268c32e2cf319d2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T06:37:36.877858Z",
     "start_time": "2025-06-19T07:18:58.771596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "accuracy score: 86.93%\n",
      "Confusion Matrix:\n",
      " [[173   0   0]\n",
      " [  9   0   0]\n",
      " [ 17   0   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.87      1.00      0.93       173\n",
      "         NEU       0.00      0.00      0.00         9\n",
      "         POS       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.87       199\n",
      "   macro avg       0.29      0.33      0.31       199\n",
      "weighted avg       0.76      0.87      0.81       199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay Tai\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Jay Tai\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Jay Tai\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Multinomial NB\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Create a pipeline with CountVectorizer, TfidfTransformer, and Multinomial Naive Bayes\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test dataset\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"MultinomialNB\")\n",
    "print(\"accuracy score: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "647cda317b364388",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:21:26.206819Z",
     "start_time": "2025-06-19T07:21:26.175328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB\n",
      "accuracy score: 86.43%\n",
      "Confusion Matrix:\n",
      " [[172   0   1]\n",
      " [  9   0   0]\n",
      " [ 17   0   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.87      0.99      0.93       173\n",
      "         NEU       0.00      0.00      0.00         9\n",
      "         POS       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.86       199\n",
      "   macro avg       0.29      0.33      0.31       199\n",
      "weighted avg       0.76      0.86      0.81       199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jay Tai\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Jay Tai\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Jay Tai\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Binomial NB\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Create a pipeline with CountVectorizer, TfidfTransformer, and Bernoulli Naive Bayes\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', BernoulliNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test dataset\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"BernoulliNB\")\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy score: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e321fde09194fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:22:00.323952Z",
     "start_time": "2025-06-19T07:21:59.369529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEU' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEU' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEU' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEU' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEU' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG']\n",
      "GradientBoostingClassifier\n",
      "accuracy score: 87.91%\n",
      "Confusion Matrix:\n",
      " [[343   2   1]\n",
      " [ 12   1   2]\n",
      " [ 29   2   5]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.89      0.99      0.94       346\n",
      "         NEU       0.20      0.07      0.10        15\n",
      "         POS       0.62      0.14      0.23        36\n",
      "\n",
      "    accuracy                           0.88       397\n",
      "   macro avg       0.57      0.40      0.42       397\n",
      "weighted avg       0.84      0.88      0.84       397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grad Boost\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Create a pipeline with CountVectorizer, TfidfTransformer, and Gradient Boosting Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test dataset\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "print(\"GradientBoostingClassifier\")\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy score: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a69f9c48110df1af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:22:14.389387Z",
     "start_time": "2025-06-19T07:22:14.154339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoostClassifier\n",
      "accuracy score: 86.43%\n",
      "Confusion Matrix:\n",
      " [[168   1   4]\n",
      " [  8   1   0]\n",
      " [ 13   1   3]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.89      0.97      0.93       173\n",
      "         NEU       0.33      0.11      0.17         9\n",
      "         POS       0.43      0.18      0.25        17\n",
      "\n",
      "    accuracy                           0.86       199\n",
      "   macro avg       0.55      0.42      0.45       199\n",
      "weighted avg       0.82      0.86      0.84       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XG Boost\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Create a label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder on the sentiment labels and transform them to numerical values\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Create a pipeline with CountVectorizer, TfidfTransformer, and XGBoost Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', XGBClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predict on the test dataset\n",
    "y_pred_encoded = pipeline.predict(X_test)\n",
    "print(\"XGBoostClassifier\")\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred_encoded)\n",
    "print(\"accuracy score: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_encoded, y_pred_encoded))\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_encoded, y_pred_encoded, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1aaf5077f50ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T06:17:05.947455Z",
     "start_time": "2025-06-23T06:17:05.873826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier\n",
      "accuracy score: 83.42%\n",
      "Confusion Matrix:\n",
      " [[161   4   8]\n",
      " [  5   1   3]\n",
      " [ 11   2   4]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.91      0.93      0.92       173\n",
      "         NEU       0.14      0.11      0.12         9\n",
      "         POS       0.27      0.24      0.25        17\n",
      "\n",
      "    accuracy                           0.83       199\n",
      "   macro avg       0.44      0.43      0.43       199\n",
      "weighted avg       0.82      0.83      0.83       199\n",
      "\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# DT\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Assuming you have already split the data into X_train, X_test, y_train, y_test\n",
    "# If not, please refer to the previous code snippets\n",
    "\n",
    "# Create a pipeline with CountVectorizer, TfidfTransformer, and Decision Tree Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test dataset\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"DecisionTreeClassifier\")\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy score: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "print(pipeline.named_steps['model'].feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e0c3265eae1591d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T06:14:08.532767900Z",
     "start_time": "2025-06-19T07:22:47.723243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "accuracy score: 87.44%\n",
      "Confusion Matrix:\n",
      " [[172   0   1]\n",
      " [  8   1   0]\n",
      " [ 15   1   1]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.88      0.99      0.93       173\n",
      "         NEU       0.50      0.11      0.18         9\n",
      "         POS       0.50      0.06      0.11        17\n",
      "\n",
      "    accuracy                           0.87       199\n",
      "   macro avg       0.63      0.39      0.41       199\n",
      "weighted avg       0.83      0.87      0.83       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RF\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Assuming you have already split the data into X_train, X_test, y_train, y_test\n",
    "# If not, please refer to the previous code snippets\n",
    "\n",
    "# Create a pipeline with CountVectorizer, TfidfTransformer, and Decision Tree Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test dataset\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"RandomForestClassifier\")\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy score: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3479e2a81c1251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T07:23:15.302446Z",
     "start_time": "2025-06-19T07:23:15.269621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG'\n",
      " 'POS' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEU' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEU' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEU' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG'\n",
      " 'NEG' 'NEG' 'NEU' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEU' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEU' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'POS' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG' 'NEG'\n",
      " 'NEG']\n",
      "KNeighborsClassifier\n",
      "accuracy score: 88.41%\n",
      "Confusion Matrix:\n",
      " [[342   2   2]\n",
      " [ 13   2   0]\n",
      " [ 27   2   7]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.90      0.99      0.94       346\n",
      "         NEU       0.33      0.13      0.19        15\n",
      "         POS       0.78      0.19      0.31        36\n",
      "\n",
      "    accuracy                           0.88       397\n",
      "   macro avg       0.67      0.44      0.48       397\n",
      "weighted avg       0.86      0.88      0.85       397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Create a pipeline with CountVectorizer, TfidfTransformer, and K-Nearest Neighbors Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test dataset\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "print(\"KNeighborsClassifier\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy score: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
