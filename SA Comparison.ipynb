{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Comparing between different Sentimental Analaysis Models\n",
    "\n",
    "Goal: To determine the best model on hand\n",
    "\n",
    "Description:  I have a csv of headliners, and fed it to GrokAI to generate a list of sentimental Scores. This will be used as the benchmark in the comparison.\n",
    "\n",
    "Steps outlined:\n",
    "1. Setup the file \"with_sentiment_100.csv\" for comparison and briefly screen through the list for outliers\n",
    "2. Run through the different models and run the data through them (We are interested to know if its positive or negative)\n",
    "3. Compare with true values\n",
    "\n",
    "Models:\n",
    "- vader\n",
    "- textblob\n",
    "- flair\n",
    "- roberta\n",
    "- distilbert\n",
    "- bertweet\n",
    "- finbert\n",
    "- deberta\n",
    "- qwen llm"
   ],
   "id": "bbeb033608f08ddd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Setup testing file",
   "id": "974369bbd4b8facb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T03:04:52.484324Z",
     "start_time": "2025-06-18T03:04:52.342138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/headlines_with_sentiment.csv\")\n",
    "df.head()"
   ],
   "id": "a848f71571d52b07",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Index                                           Headline Sentiment_label\n",
       "0      0   Johnson is asking Santa for a Christmas recovery             POS\n",
       "1      1  ‘I now fear the worst’: four grim tales of wor...             NEG\n",
       "2      2  Five key areas Sunak must tackle to serve up e...             NEU\n",
       "3      3  Covid-19 leaves firms ‘fatally ill-prepared’ f...             NEG\n",
       "4      4  The Week in Patriarchy Bacardi's 'lady vodka':...             NEG"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Johnson is asking Santa for a Christmas recovery</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>‘I now fear the worst’: four grim tales of wor...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Five key areas Sunak must tackle to serve up e...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Covid-19 leaves firms ‘fatally ill-prepared’ f...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Week in Patriarchy Bacardi's 'lady vodka':...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T03:05:00.065040Z",
     "start_time": "2025-06-18T03:05:00.052523Z"
    }
   },
   "cell_type": "code",
   "source": "df.shape",
   "id": "b46b169d6a7299ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1984, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.info()",
   "id": "4169c2a32d507ae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe()",
   "id": "ac756655313fe851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T03:05:02.064695Z",
     "start_time": "2025-06-18T03:05:02.060586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "headline = list(df['Headline'])\n",
    "len(headline)"
   ],
   "id": "4b5c64b58a3649c7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1984"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Run the list through different models",
   "id": "b7999155b16b3afc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T03:05:16.276851Z",
     "start_time": "2025-06-18T03:05:06.919235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set up\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str) or text is None:\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    test_sentence = \" \".join(cleaned_tokens)\n",
    "    return test_sentence\n",
    "\n",
    "processed_headline = list(map(preprocess, headline))"
   ],
   "id": "548065da08211f47",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/jaytai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaytai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jaytai/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Prebuilt Vader sentiment package (NaiveBayes model) - Done\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "result_vader = []\n",
    "threshold_upper = .05\n",
    "threshold_lower = -.05\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in processed_headline:\n",
    "    score = analyzer.polarity_scores(sentence)\n",
    "    result_vader.append(score['compound'])\n",
    "result_vader = ['POS' if s >= threshold_upper else 'NEG' if s <= threshold_lower else 'NEU' for s in result_vader]\n",
    "print(result_vader[:10])\n",
    "print(len(result_vader))\n",
    "pd.DataFrame(result_vader).to_csv(\"data/result_vader.csv\")\n",
    "\n",
    "# to determine the threshold"
   ],
   "id": "e45f5d28bbc253e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Prebuilt Textblob sentiment package - Done\n",
    "\n",
    "from textblob import TextBlob\n",
    "result_tb = []\n",
    "threshold_upper = .05\n",
    "threshold_lower = -.05\n",
    "for sentence in processed_headline:\n",
    "    result_tb.append(TextBlob(sentence).sentiment.polarity)\n",
    "result_tb = ['POS' if s >= threshold_upper else 'NEG' if s <= threshold_lower else 'NEU' for s in result_tb]\n",
    "print(result_tb[:10])\n",
    "print(len(result_tb))\n",
    "pd.DataFrame(result_tb).to_csv(\"data/result_tb.csv\")\n",
    "\n",
    "# to determine the threshold"
   ],
   "id": "dabb924578b70adc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T03:06:00.252804Z",
     "start_time": "2025-06-18T03:05:24.392785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Prebuilt Flair sentiment package/Model - Done\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "result_flair = []\n",
    "tagger = Classifier.load('sentiment')\n",
    "\n",
    "for sentence in processed_headline:\n",
    "    sentence = Sentence(sentence)\n",
    "    tagger.predict(sentence)\n",
    "    value = sentence.labels[0].value\n",
    "    result_flair.append(value)\n",
    "result_flair = ['POS' if s == 'POSITIVE' else 'NEG' if s == 'NEGATIVE' else 'NEU' for s in result_flair]\n",
    "print(result_flair[:10])\n",
    "print(len(result_flair))\n",
    "pd.DataFrame(result_flair).to_csv(\"data/result_flair.csv\")"
   ],
   "id": "b2947c9fcd7945dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['POS', 'NEG', 'POS', 'NEG', 'NEG', 'NEG', 'POS', 'NEG', 'NEG', 'NEG']\n",
      "1984\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# setup for HuggingFace Transformers\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "set_seed(999)"
   ],
   "id": "9a198a7537747514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# RoBERTa - Done\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "\n",
    "result_roberta = []\n",
    "for sentence in processed_headline:\n",
    "    temp = classifier(sentence)\n",
    "    result_roberta.append(temp[0]['label'])\n",
    "result_roberta = ['POS' if s == 'positive' else 'NEG' if s == 'negative' else 'NEU' for s in result_roberta]\n",
    "print(result_roberta[:10])\n",
    "print(len(result_roberta))\n",
    "pd.DataFrame(result_roberta).to_csv(\"data/result_roberta.csv\")"
   ],
   "id": "81ee78c078282156",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# distilBERT - Done\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "result_dis = []\n",
    "for sentence in processed_headline:\n",
    "    temp = classifier(sentence)\n",
    "    result_dis.append(temp[0]['label'])\n",
    "result_dis = ['POS' if s == 'POSITIVE' else 'NEG' if s == 'NEGATIVE' else 'NEU' for s in result_dis]\n",
    "print(result_dis[:10])\n",
    "print(len(result_dis))\n",
    "pd.DataFrame(result_dis).to_csv(\"data/result_distilbert.csv\")"
   ],
   "id": "1529a63db0d77e0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Bertweet - Done\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "\n",
    "result_bertweet = []\n",
    "for sentence in processed_headline:\n",
    "    result_bertweet.append(classifier(sentence)[0]['label'])\n",
    "print(result_bertweet[:10])\n",
    "print(len(result_bertweet))\n",
    "pd.DataFrame(result_bertweet).to_csv(\"data/result_bertweet.csv\")"
   ],
   "id": "682c686030bb5642",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# finBERT - Done\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
    "\n",
    "result_finbert = []\n",
    "for sentence in processed_headline:\n",
    "    temp = classifier(sentence)\n",
    "    result_finbert.append(temp[0]['label'])\n",
    "result_finbert = ['POS' if s == 'positive' else 'NEG' if s == 'negative' else 'NEU' for s in result_finbert]\n",
    "print(result_finbert[:10])\n",
    "print(len(result_finbert))\n",
    "pd.DataFrame(result_finbert).to_csv(\"data/result_finbert.csv\")"
   ],
   "id": "6dad42ce7ea0e585",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # LLM QWEN 8gb - Abandoned (ratelimit exceeded)\n",
    "# import time\n",
    "# from openai import OpenAI\n",
    "#\n",
    "# client = OpenAI(\n",
    "#   base_url=\"https://openrouter.ai/api/v1\",\n",
    "#   api_key=\"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\",\n",
    "# )\n",
    "#\n",
    "# result_ai = []\n",
    "# DELAY = 1  # Start with 1 second delay, adjust as needed\n",
    "# for i, sentence in enumerate(processed_headline):\n",
    "#     try:\n",
    "#         completion = client.chat.completions.create(\n",
    "#             extra_body={},\n",
    "#             model=\"deepseek/deepseek-r1-0528-qwen3-8b:free\",\n",
    "#             messages=[{\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": f\"Only give me a sentimental analysis value of NEG (negative), POS (positive), NEU (neutral) for the following sentence {sentence}. Dont add anything else to the output\"\n",
    "#             }]\n",
    "#         )\n",
    "#         result_ai.append(completion.choices[0].message.content)\n",
    "#\n",
    "#         # Print progress every 10 requests\n",
    "#         if i % 10 == 0:\n",
    "#             print(f\"Processed {i+1}/{len(processed_headline)} requests\")\n",
    "#\n",
    "#         # Add delay between requests\n",
    "#         time.sleep(DELAY)\n",
    "#\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error on request {i+1}: {str(e)}\")\n",
    "#         # If rate limited, increase delay and retry\n",
    "#         DELAY += 1\n",
    "#         time.sleep(DELAY)\n",
    "#         continue\n",
    "#\n",
    "# print(result_ai[:10])\n",
    "# print(len(result_ai))\n",
    "# pd.DataFrame(result_ai).to_csv(\"data/result_ai.csv\")"
   ],
   "id": "a86f44bee2350198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparing with the benchmark",
   "id": "8326ec41f38b5d8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# visualising the data\n",
    "\n"
   ],
   "id": "12c9bff0501c05ef",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
