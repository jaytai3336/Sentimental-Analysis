{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Comparing between different Sentimental Analaysis Models\n",
    "\n",
    "Goal: To determine the best model on hand\n",
    "\n",
    "Description:  I have a csv of headliners, and fed it to GrokAI to generate a list of sentimental Scores. This will be used as the benchmark in the comparison.\n",
    "\n",
    "Steps outlined:\n",
    "1. Setup the file \"with_sentiment_100.csv\" for comparison and briefly screen through the list for outliers\n",
    "2. Run through the different models and run the data through them (We are interested to know if its positive or negative)\n",
    "3. Compare with true values\n",
    "\n",
    "Models:\n",
    "- vader\n",
    "- textblob\n",
    "- flair\n",
    "- roberta\n",
    "- distilbert\n",
    "- bertweet\n",
    "- finbert\n",
    "- deberta\n",
    "- qwen llm"
   ],
   "id": "bbeb033608f08ddd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Setup testing file",
   "id": "974369bbd4b8facb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/headlines_with_sentiment.csv\")\n",
    "df.head()"
   ],
   "id": "a848f71571d52b07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.shape",
   "id": "b46b169d6a7299ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.info()",
   "id": "4169c2a32d507ae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe()",
   "id": "ac756655313fe851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "headline = list(df['Headline'])\n",
    "len(headline)"
   ],
   "id": "4b5c64b58a3649c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Run the list through different models",
   "id": "b7999155b16b3afc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# set up\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    if not isinstance(text, str) or text is None:\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    test_sentence = \" \".join(cleaned_tokens)\n",
    "    return test_sentence\n",
    "\n",
    "processed_headline = list(map(preprocess, headline))"
   ],
   "id": "548065da08211f47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Prebuilt Vader sentiment package (NaiveBayes model) - Done\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "result_vader = []\n",
    "threshold_upper = .05\n",
    "threshold_lower = -.05\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in processed_headline:\n",
    "    score = analyzer.polarity_scores(sentence)\n",
    "    result_vader.append(score['compound'])\n",
    "result_vader = ['POS' if s >= threshold_upper else 'NEG' if s <= threshold_lower else 'NEU' for s in result_vader]\n",
    "print(result_vader[:10])\n",
    "print(len(result_vader))\n",
    "pd.DataFrame(result_vader).to_csv(\"data/result_vader.csv\")\n",
    "\n",
    "# to determine the threshold"
   ],
   "id": "e45f5d28bbc253e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Prebuilt Textblob sentiment package - Done\n",
    "\n",
    "from textblob import TextBlob\n",
    "result_tb = []\n",
    "threshold_upper = .05\n",
    "threshold_lower = -.05\n",
    "for sentence in processed_headline:\n",
    "    result_tb.append(TextBlob(sentence).sentiment.polarity)\n",
    "result_tb = ['POS' if s >= threshold_upper else 'NEG' if s <= threshold_lower else 'NEU' for s in result_tb]\n",
    "print(result_tb[:10])\n",
    "print(len(result_tb))\n",
    "pd.DataFrame(result_tb).to_csv(\"data/result_tb.csv\")\n",
    "\n",
    "# to determine the threshold"
   ],
   "id": "dabb924578b70adc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Prebuilt Flair sentiment package/Model - Done\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "result_flair = []\n",
    "tagger = Classifier.load('sentiment')\n",
    "\n",
    "for sentence in processed_headline:\n",
    "    sentence = Sentence(sentence)\n",
    "    tagger.predict(sentence)\n",
    "    value = sentence.labels[0].value\n",
    "    result_flair.append(value)\n",
    "result_flair = ['POS' if s == 'POSITIVE' else 'NEG' if s == 'NEGATIVE' else 'NEU' for s in result_flair]\n",
    "print(result_flair[:10])\n",
    "print(len(result_flair))\n",
    "pd.DataFrame(result_flair).to_csv(\"data/result_flair.csv\")"
   ],
   "id": "b2947c9fcd7945dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# setup for HuggingFace Transformers\n",
    "\n",
    "from transformers import pipeline, set_seed\n",
    "set_seed(999)"
   ],
   "id": "9a198a7537747514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# RoBERTa - Done\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model='cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "\n",
    "result_roberta = []\n",
    "for sentence in processed_headline:\n",
    "    temp = classifier(sentence)\n",
    "    result_roberta.append(temp[0]['label'])\n",
    "result_roberta = ['POS' if s == 'positive' else 'NEG' if s == 'negative' else 'NEU' for s in result_roberta]\n",
    "print(result_roberta[:10])\n",
    "print(len(result_roberta))\n",
    "pd.DataFrame(result_roberta).to_csv(\"data/result_roberta.csv\")"
   ],
   "id": "81ee78c078282156",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# distilBERT - Done\n",
    "\n",
    "classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "result_dis = []\n",
    "for sentence in processed_headline:\n",
    "    temp = classifier(sentence)\n",
    "    result_dis.append(temp[0]['label'])\n",
    "result_dis = ['POS' if s == 'POSITIVE' else 'NEG' if s == 'NEGATIVE' else 'NEU' for s in result_dis]\n",
    "print(result_dis[:10])\n",
    "print(len(result_dis))\n",
    "pd.DataFrame(result_dis).to_csv(\"data/result_distilbert.csv\")"
   ],
   "id": "1529a63db0d77e0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Bertweet - Done\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "\n",
    "result_bertweet = []\n",
    "for sentence in processed_headline:\n",
    "    result_bertweet.append(classifier(sentence)[0]['label'])\n",
    "print(result_bertweet[:10])\n",
    "print(len(result_bertweet))\n",
    "pd.DataFrame(result_bertweet).to_csv(\"data/result_bertweet.csv\")"
   ],
   "id": "682c686030bb5642",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# finBERT - Done\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
    "\n",
    "result_finbert = []\n",
    "for sentence in processed_headline:\n",
    "    temp = classifier(sentence)\n",
    "    result_finbert.append(temp[0]['label'])\n",
    "result_finbert = ['POS' if s == 'positive' else 'NEG' if s == 'negative' else 'NEU' for s in result_finbert]\n",
    "print(result_finbert[:10])\n",
    "print(len(result_finbert))\n",
    "pd.DataFrame(result_finbert).to_csv(\"data/result_finbert.csv\")"
   ],
   "id": "6dad42ce7ea0e585",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # LLM QWEN 8gb - Abandoned (ratelimit exceeded)\n",
    "# import time\n",
    "# from openai import OpenAI\n",
    "#\n",
    "# client = OpenAI(\n",
    "#   base_url=\"https://openrouter.ai/api/v1\",\n",
    "#   api_key=\"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\",\n",
    "# )\n",
    "#\n",
    "# result_ai = []\n",
    "# DELAY = 1  # Start with 1 second delay, adjust as needed\n",
    "# for i, sentence in enumerate(processed_headline):\n",
    "#     try:\n",
    "#         completion = client.chat.completions.create(\n",
    "#             extra_body={},\n",
    "#             model=\"deepseek/deepseek-r1-0528-qwen3-8b:free\",\n",
    "#             messages=[{\n",
    "#                 \"role\": \"user\",\n",
    "#                 \"content\": f\"Only give me a sentimental analysis value of NEG (negative), POS (positive), NEU (neutral) for the following sentence {sentence}. Dont add anything else to the output\"\n",
    "#             }]\n",
    "#         )\n",
    "#         result_ai.append(completion.choices[0].message.content)\n",
    "#\n",
    "#         # Print progress every 10 requests\n",
    "#         if i % 10 == 0:\n",
    "#             print(f\"Processed {i+1}/{len(processed_headline)} requests\")\n",
    "#\n",
    "#         # Add delay between requests\n",
    "#         time.sleep(DELAY)\n",
    "#\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error on request {i+1}: {str(e)}\")\n",
    "#         # If rate limited, increase delay and retry\n",
    "#         DELAY += 1\n",
    "#         time.sleep(DELAY)\n",
    "#         continue\n",
    "#\n",
    "# print(result_ai[:10])\n",
    "# print(len(result_ai))\n",
    "# pd.DataFrame(result_ai).to_csv(\"data/result_ai.csv\")"
   ],
   "id": "a86f44bee2350198",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparing with the benchmark",
   "id": "8326ec41f38b5d8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# visualising the data\n",
    "\n"
   ],
   "id": "12c9bff0501c05ef",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
